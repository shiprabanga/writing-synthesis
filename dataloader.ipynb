{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import os\n",
    "import pickle\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess():\n",
    "    def __init__(self):\n",
    "        self.rootDir = \"./data\"\n",
    "        self.batch_size = 2\n",
    "        self.index_pointer = 0\n",
    "        self.timesteps = 150\n",
    "        \n",
    "        stroke_dir = self.rootDir\n",
    "        data_file = os.path.join(self.rootDir, \"strokes_training_data.cpkl\")\n",
    "        self.process(stroke_dir, data_file)\n",
    "        self.read_processed(data_file)\n",
    "        self.init_batch_comp()\n",
    "    \n",
    "    # Read processed data from .cpkl file.\n",
    "    def read_processed(self, data_file):\n",
    "        # Opening in read mode\n",
    "        f = open(data_file, 'rb')\n",
    "        self.raw_stroke_data = pickle.load(f)\n",
    "        f.close()\n",
    "        \n",
    "        self.valid_stroke_data = []\n",
    "        self.stroke_data = []\n",
    "        \n",
    "        for i in range(len(self.raw_stroke_data)):\n",
    "            data = self.raw_stroke_data[i]\n",
    "            if i%20 == 0:\n",
    "                self.valid_stroke_data.append(data)\n",
    "            else:\n",
    "                self.stroke_data.append(data)\n",
    "        # TODO : Do normalisation here.\n",
    "        self.num_batches = int(len(self.stroke_data)/self.batch_size)\n",
    "        print(\"Number of data examples:\",  len(self.stroke_data))\n",
    "        print(\"Batch size for dataset\", self.num_batches)\n",
    "        \n",
    "    def init_batch_comp(self):\n",
    "        self.index_perm = np.random.permutation(len(self.stroke_data))\n",
    "        self.index_pointer = 0\n",
    "    \n",
    "    def get_validation_data(self):\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            index = i%len(self.valid_stroke_data)\n",
    "            data = self.valid_stroke_data[index]\n",
    "            x_batch.append(np.copy(data[:self.tsteps]))\n",
    "            y_batch.append(np.copy(data[1:self.tsteps+1]))\n",
    "            return x_batch, y_batch\n",
    "    \n",
    "    def get_next_batch(self):\n",
    "        # Iterate for batch_size times to get a batch of batch_size points\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for i in range(self.batch_size):\n",
    "            # Pick strokes data randomly from each file\n",
    "            data = self.stroke_data[self.index_perm[self.index_pointer]]\n",
    "            x_batch.append(np.copy(data[:self.timesteps]))\n",
    "            y_batch.append(np.copy(data[1:self.timesteps+1]))\n",
    "            self.index_pointer += 1\n",
    "            if(self.index_pointer >= len(self.stroke_data)):\n",
    "                self.init_batch_comp()     \n",
    "        return x_batch, y_batch         \n",
    "            \n",
    "        \n",
    "    def process(self, rootDir, data_file):\n",
    "        # Function that outputs linestrokes given filepath.    \n",
    "        def convert_linestroke_file_to_array(filepath):\n",
    "            strokeFile = ET.parse(filepath)\n",
    "            root = strokeFile.getroot()\n",
    "            x_min_offset = -1000000\n",
    "            y_min_offset = -1000000\n",
    "            y_height = 0\n",
    "            for i in range(1,4):\n",
    "                x_min_offset = min(x_min_offset, float(root[0][i].attrib['x']))\n",
    "                y_min_offset = min(y_min_offset, float(root[0][i].attrib['y']))\n",
    "                y_height = max(y_height, float(root[0][i].attrib['y']))\n",
    "            #TODO(add normalization)\n",
    "            y_height -= y_min_offset\n",
    "            x_min_offset -=100.0\n",
    "            y_min_offset -=100.0\n",
    "            strokeSet = root[1]\n",
    "            allStrokes = []\n",
    "            for i in range(len(strokeSet)):\n",
    "                points = []\n",
    "                for point in strokeSet[i]:\n",
    "                    points.append([(float(point.attrib['x']) - x_min_offset), (float(point.attrib['y']) - y_min_offset)])\n",
    "                allStrokes.append(points)\n",
    "            return allStrokes    \n",
    "                \n",
    "    \n",
    "        def get_all_files():\n",
    "            rootDir = \"./data\"\n",
    "            filePaths = []\n",
    "            for dirpath, dirnames, filenames in os.walk(rootDir):\n",
    "                for file in filenames:\n",
    "                    filePaths.append(os.path.join(dirpath, file))\n",
    "            return filePaths\n",
    "\n",
    "    #for file in get_all_files(rootdir):\n",
    "     #   convert_linestroke_file_to_array(file)\n",
    "        \n",
    "    # Function to convert strokes to inputStrokeMatrix\n",
    "        def strokes_to_input_matrix(strokes):\n",
    "            strokeMatrix = []\n",
    "            prev_x = 0\n",
    "            prev_y = 0\n",
    "            for stroke in strokes:\n",
    "                for num_point in range(len(stroke)):\n",
    "                    x = stroke[num_point][0] - prev_x\n",
    "                    y = stroke[num_point][1] - prev_y\n",
    "                    prev_x = stroke[num_point][0]\n",
    "                    prev_y = stroke[num_point][1]\n",
    "                    z = 0\n",
    "                    if (num_point == len(stroke)-1):\n",
    "                        z = 1\n",
    "                    example = [x,y,z]\n",
    "                    strokeMatrix.append(example)\n",
    "            return strokeMatrix\n",
    "        \n",
    "        allFiles = get_all_files()\n",
    "        strokes = []\n",
    "        counter = 0\n",
    "        for file in allFiles:\n",
    "            if file[-3:] == \"xml\" and 'a01-' in file:\n",
    "                counter = counter + 1\n",
    "                stroke = strokes_to_input_matrix(convert_linestroke_file_to_array(file))\n",
    "                strokes.append(stroke)\n",
    "            assert len(strokes) == counter    \n",
    "        f = open(data_file,\"wb\")\n",
    "        pickle.dump(strokes, f, protocol=2)\n",
    "        f.close()\n",
    "        print(\"Saved {} lines\", len(strokes))\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataProcess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.init_batch_comp()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_plot(strokes, title):\n",
    "    plt.figure(figsize=(20,2))\n",
    "    eos_preds = np.where(strokes[:,-1] == 1)\n",
    "    eos_preds = [0] + list(eos_preds[0]) + [-1] #add start and end indices\n",
    "    for i in range(len(eos_preds)-1):\n",
    "        start = eos_preds[i]+1\n",
    "        stop = eos_preds[i+1]\n",
    "        plt.plot(strokes[start:stop,0], strokes[start:stop,1],'b-', linewidth=2.0)\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = d.get_next_batch()\n",
    "for i in range(d.batch_size):\n",
    "    r = x[i]\n",
    "    strokes = r.copy()\n",
    "    strokes[:,:-1] = np.cumsum(r[:,:-1], axis=0)\n",
    "    line_plot(strokes, \"Batch plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is a batch\n",
    "# x[0] is one training example inside batch\n",
    "# What does x[0] look like\n",
    "# It looks like tstep points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
